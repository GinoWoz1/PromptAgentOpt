# prompt_optimizer/models.py

"""
Data models and type definitions for the prompt optimization system.
(Updated for simplified Generate -> Evaluate -> Aggregate workflow)
"""

import operator
from typing import Annotated, List, Dict, Any, TypedDict, Optional, Union
import uuid

# -----------------------------------------------------------------------
# Core Data Structures for Generalization
# -----------------------------------------------------------------------

class InputExample(TypedDict):
    """Represents a single input example from the dataset."""
    id: str
    data: Dict[str, Any]

class PromptCandidate(TypedDict):
    """Represents a single prompt generated by the Optimizer LLM."""
    candidate_id: str
    prompt_text: str  # Template
    iteration: int
    embedding: List[float]
    is_elite: bool  # Flag to identify if this candidate was carried over as an elite

# ExecutionResult is removed as it's now internal to the evaluation node.

class Vote(TypedDict):
    """
    A single evaluation by a voter LLM.
    In the simplified workflow, this object also carries the execution details for traceability.
    """
    vote_id: str
    voter_id: str
    candidate_id: str
    input_example_id: str
    score: float
    critique: str
    # Fields required for traceability in the aggregation step:
    execution_id: str # Unique ID for the execution event
    executed_prompt_text: str
    output: str
    input_example_data: Dict[str, Any]


class PerformanceExample(TypedDict):
    """
    Captures the details of a specific execution based on voter consensus.
    Used for tracking the best and worst performing examples across iterations.
    """
    candidate_id: str
    prompt_template: str
    input_example_id: str
    consensus_normalized_score: float
    consensus_raw_score: float
    executed_prompt_text: str
    output: str
    critiques: List[str]

class IterationResult(TypedDict):
    """Aggregated results for a candidate prompt across an entire iteration."""
    candidate_id: str
    prompt_text: str  # The template
    iteration: int
    aggregate_score: float
    raw_average_score: float

# -----------------------------------------------------------------------
# Detailed Traceability
# -----------------------------------------------------------------------

class ExecutionTrace(TypedDict):
    """
    Combines the execution result with the aggregated voting results for full traceability.
    """
    iteration: int
    candidate_id: str
    input_example_id: str
    prompt_template: str
    executed_prompt_text: str
    output: str
    avg_normalized_score: float
    avg_raw_score: float
    # Note: Votes here should be the cleaned version (without redundant execution details)
    votes: List[Dict[str, Any]]

# -----------------------------------------------------------------------
# Prompt Generation Traceability
# -----------------------------------------------------------------------

class PromptGenerationTrace(TypedDict):
    """
    Tracks the details of the prompt generation process within a single iteration/attempt.
    """
    iteration: int
    attempt: Union[int, str]  # Can be an integer (1, 2, 3) or a string ("FALLBACK")
    timestamp: str
    optimizer_model: str
    # Inputs to the generation process
    input_synthesized_critiques: str
    # Summary of scores/IDs used as input (Top N)
    input_performance_history_summary: List[Dict[str, Any]]
    # Parameters
    num_requested: int
    # Results
    num_generated: int
    num_accepted: int  # Total accepted in this attempt (includes elites if first attempt)
    num_rejected_similarity: int
    # Details about why prompts were rejected (text, similarity score, reason)
    rejected_prompts: List[Dict[str, Any]]
    # Details about accepted prompts (ID, text, is_elite, similarity score)
    accepted_prompts: List[Dict[str, Any]]
    llm_call_successful: bool
    error_message: Optional[str]

# -----------------------------------------------------------------------
# Task Definitions (Inputs for parallel nodes)
# -----------------------------------------------------------------------

# Replaces ExecutionTask and VotingTask
class EvaluationTask(TypedDict):
    """Defines a task for executing a prompt candidate and evaluating the result."""
    candidate: PromptCandidate
    input_example: InputExample
    actor_model: str
    # Added voting configuration required by the combined node
    voter_ensemble: List[str]
    target_task_description: str

# -----------------------------------------------------------------------
# Graph State
# -----------------------------------------------------------------------

class OptimizationState(TypedDict):
    """
    The main state for tracking the optimization workflow.
    """

    # Configuration and Input Data
    max_iterations: int
    target_task_description: str
    N_CANDIDATES: int
    MINI_BATCH_SIZE: int
    input_dataset: List[InputExample]

    # Early Stopping Configuration
    es_min_iterations: int
    es_patience: int
    es_threshold_percentage: float

    # Models Configuration
    optimizer_model: str
    voter_ensemble: List[str]
    actor_model: str

    # Iteration State
    current_iteration: int
    current_candidates: Dict[str, PromptCandidate]
    current_mini_batch: List[InputExample]

    # Accumulators for current iteration
    # current_execution_results is removed.
    # Votes are accumulated automatically by LangGraph during the parallel evaluation step.
    current_votes: Annotated[List[Vote], operator.add]

    # History and State Tracking
    all_tested_prompts: Dict[str, PromptCandidate]
    history: List[IterationResult]
    synthesized_critiques: str
    best_result: Optional[IterationResult]
    iteration_best_score_history: List[float]

    # Global Performance Tracking
    global_best_example: Optional[PerformanceExample]
    global_worst_example: Optional[PerformanceExample]

    # Detailed execution traces across all iterations for analysis
    execution_trace_history: Annotated[List[ExecutionTrace], operator.add]

    # Configuration for prompt tracing (file path provided by runner)
    prompt_trace_file_path: Optional[str]