# defines the data model and outputs

import operator
from typing import Annotated, Literal, List, Dict, Any
from pydantic import BaseModel, Field, TypedDict, Optional
import uuid 

# -----------------------------------------------------------------------
# Core Data Structures
# -----------------------------------------------------------------------

class PromptCandidate(TypedDict):
    """ Represents a single prompt generated by the Optimizer LLM."""
    canditate_id: str = Field(default_factory = lambda: str(uuid.uuid4))[:8]
    prompt_text: str
    iteration: int 
    embedding: List[float]

class ExecutionResult(TypedDict):
    """ The result of running a candidate prompt against the target task."""
    execution_id: str 
    candidate_id: str 
    output: str
    # can other result metrics as needed

class Vote(TypedDict):
    """ A single evaluation by a voter LLM"""
    voter_id: str # e.g model name 
    candidate_id: str 
    score: float  # raw score (e.g 1-10)
    critique: str  # rational for the score
    execution_result_id: str

class IterationResult(TypedDict):
    candidate_id: str
    prompt_text: str
    iteration: int 
    aggregate_score: float # The normalized z-score average
    raw_average_score: float
    votes: List[Vote]

class EvaludatedResult(BaseModel):
    prompt_id: str 
    prompt_text: str 
    aggregated_score: float 
    votes: List[Vote]

# -----------------------------------------------------------------------
# Task Definitions (Inputs for parallel nodes)
# -----------------------------------------------------------------------
# These are the payloads sent via the routers (Send)

class ExecutionTask(TypedDict):
    candidate: PromptCandidate

class VotingTask(TypedDict):
    executionResult: ExecutionResult
    voter_id: str 

# -----------------------------------------------------------------------
# Graph State
# -----------------------------------------------------------------------

class OptimizationState(TypedDict):
    "The main state for tracking the optimization workflow"

    # Configuration
    max_iterations: int
    voter_ensemble: List[str]
    target_task_description: str 
    N_Candidates: int # candidates per iterations

    # Iteration State
    current_iteration: int
    current_candidates: Dict[str, PromptCandidate]

    # Accumulators: LangGraph automatically accumulates results from parallel nodes here
    # We use annotated with operators.add for this purpose
    
    current_execution_results: Annotated[List[ExecutionResult], operator.add]
    current_votes: Annotated[List[Vote], operator.add]

    # History (persistent across iterations)
    # Stores results sorted by score

    history: List[IterationResult]

    # stores all tested prompt (including embeddings) for novelty checking and lookups
    all_tested_prompt: Dict[str, PromptCandidate]

    # Synthesized critiques from the previous iteration
    synthesized_critiques: str 

    # Best Result
    best_result: Optional[IterationResult]

