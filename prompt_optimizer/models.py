# prompt_optimizer/models.py

import operator
# Removed pydantic imports as they are not strictly needed for TypedDicts here.
from typing import Annotated
from typing import List, Dict, Any, TypedDict, Optional
import uuid

# -----------------------------------------------------------------------
# Core Data Structures for Generalization
# -----------------------------------------------------------------------

# NEW: Represents an input example from the user's dataset
class InputExample(TypedDict):
    id: str
    data: Dict[str, Any] # The actual data (e.g., {"comment": "..."})

class PromptCandidate(TypedDict):
    """ Represents a single prompt generated by the Optimizer LLM."""
    candidate_id: str
    prompt_text: str # This is now a template (e.g., "Analyze this: {comment}")
    iteration: int
    embedding: List[float]

class ExecutionResult(TypedDict):
    """ The result of running a candidate prompt against a SINGLE input example."""
    execution_id: str
    candidate_id: str
    input_example_id: str # NEW: Track which input this result corresponds to
    output: str

class Vote(TypedDict):
    """ A single evaluation by a voter LLM on a SINGLE execution result."""
    vote_id: str
    voter_id: str
    candidate_id: str
    input_example_id: str # NEW: Track which input this vote corresponds to
    score: float
    critique: str
    execution_result_id: str

class IterationResult(TypedDict):
    # Represents the aggregated performance of a prompt across the mini-batch
    candidate_id: str
    prompt_text: str
    iteration: int
    aggregate_score: float # The average normalized Z-score across the mini-batch
    raw_average_score: float

# -----------------------------------------------------------------------
# Task Definitions (Inputs for parallel nodes)
# -----------------------------------------------------------------------

class ExecutionTask(TypedDict):
    candidate: PromptCandidate
    input_example: InputExample # NEW: The specific input to test against
    actor_model: str

class VotingTask(TypedDict):
    execution_result: ExecutionResult
    voter_id: str
    target_task_description: str
    prompt_text: str
    input_example_data: Dict[str, Any] # NEW: Provide input context to the voter

# -----------------------------------------------------------------------
# Graph State
# -----------------------------------------------------------------------

class OptimizationState(TypedDict):
    """The main state for tracking the optimization workflow"""

    # Configuration
    max_iterations: int
    target_task_description: str
    N_CANDIDATES: int
    MINI_BATCH_SIZE: int # NEW: Configuration for generalization (K)

    # Input Data
    input_dataset: List[InputExample] # NEW: The dataset provided by the user

    # Models Configuration
    optimizer_model: str
    voter_ensemble: List[str]
    actor_model: str

    # Iteration State
    current_iteration: int
    current_candidates: Dict[str, PromptCandidate]
    current_mini_batch: List[InputExample] # NEW: The inputs sampled for this iteration

    # Accumulators (N * K * M parallelism)
    # FIX: Removed Annotated[..., operator.add]. When using parallel dispatch (Send),
    # LangGraph handles accumulation of list updates automatically. Explicitly using
    # operator.add prevents resetting the list in the generate_prompts node (as operator.add(existing, []) = existing),
    # leading to votes accumulating across iterations.
    current_execution_results: Annotated[List[ExecutionResult], operator.add]
    current_votes: Annotated[List[Vote], operator.add]

    # History
    all_tested_prompts: Dict[str, PromptCandidate]
    history: List[IterationResult]

    # Synthesized critiques from the previous iteration
    synthesized_critiques: str

    # Best Result
    best_result: Optional[IterationResult]