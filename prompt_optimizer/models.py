# prompt_optimizer/models.py

import operator
from typing import Annotated
from typing import List, Dict, Any, TypedDict, Optional
import uuid

# -----------------------------------------------------------------------
# Core Data Structures for Generalization
# -----------------------------------------------------------------------

class InputExample(TypedDict):
  id: str
  data: Dict[str, Any]

class PromptCandidate(TypedDict):
  """Represents a single prompt generated by the Optimizer LLM."""
  candidate_id: str
  prompt_text: str  # Template
  iteration: int
  embedding: List[float]
  # NEW (Q1): Flag to identify if this candidate was carried over as an elite
  is_elite: bool

class ExecutionResult(TypedDict):
  """The result of running a candidate prompt against a SINGLE input example."""
  execution_id: str
  candidate_id: str
  input_example_id: str
  input_example_data: Dict[str, Any]
  executed_prompt_text: str  # Interpolated prompt
  output: str

class Vote(TypedDict):
  """A single evaluation by a voter LLM on a SINGLE execution result."""
  vote_id: str
  voter_id: str
  candidate_id: str
  input_example_id: str
  score: float
  critique: str
  execution_result_id: str

# (PerformanceExample and IterationResult remain the same structure)
class PerformanceExample(TypedDict):
  """
  Captures the details of a specific execution based on voter consensus.
  """
  candidate_id: str
  prompt_template: str
  input_example_id: str
  consensus_normalized_score: float
  consensus_raw_score: float
  executed_prompt_text: str
  output: str
  critiques: List[str]

class IterationResult(TypedDict):
  candidate_id: str
  prompt_text: str  # The template
  iteration: int
  aggregate_score: float
  raw_average_score: float

# -----------------------------------------------------------------------
# NEW (Q2): Detailed Traceability
# -----------------------------------------------------------------------

class ExecutionTrace(TypedDict):
  """
  Combines the execution result with the aggregated voting results for full traceability.
  """
  iteration: int
  candidate_id: str
  input_example_id: str
  prompt_template: str
  executed_prompt_text: str
  output: str
  avg_normalized_score: float
  avg_raw_score: float
  votes: List[Vote]  # All individual votes for this execution

# -----------------------------------------------------------------------
# Task Definitions (Inputs for parallel nodes)
# -----------------------------------------------------------------------

class ExecutionTask(TypedDict):
  candidate: PromptCandidate
  input_example: InputExample
  actor_model: str

class VotingTask(TypedDict):
  execution_result: ExecutionResult
  voter_id: str
  target_task_description: str
  prompt_text: str
  input_example_data: Dict[str, Any]

# -----------------------------------------------------------------------
# Graph State
# -----------------------------------------------------------------------

class OptimizationState(TypedDict):
  """The main state for tracking the optimization workflow"""

  # Configuration and Input Data
  max_iterations: int
  target_task_description: str
  N_CANDIDATES: int
  MINI_BATCH_SIZE: int
  input_dataset: List[InputExample]

  # NEW: Early Stopping Configuration
  es_min_iterations: int
  es_patience: int
  es_threshold_percentage: float

  # Models Configuration
  optimizer_model: str
  voter_ensemble: List[str]
  actor_model: str

  # Iteration State
  current_iteration: int
  current_candidates: Dict[str, PromptCandidate]
  current_mini_batch: List[InputExample]

  # Accumulators
  current_execution_results: Annotated[List[ExecutionResult], operator.add]
  current_votes: Annotated[List[Vote], operator.add]

  # History
  all_tested_prompts: Dict[str, PromptCandidate]
  history: List[IterationResult]
  synthesized_critiques: str
  best_result: Optional[IterationResult]

  iteration_best_score_history: List[float]

  # Global Tracking for Traceability
  global_best_example: Optional[PerformanceExample]
  global_worst_example: Optional[PerformanceExample]

  # NEW (Q2): Accumulator for detailed execution traces across all iterations
  execution_trace_history: Annotated[List[ExecutionTrace], operator.add]
